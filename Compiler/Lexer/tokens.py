"""

This file contains functionality related to tokens that the lexer will use to tokenize the input.

"""

from enum import Enum

class TokenType(Enum):

    """

    This class is an enumeration of all the token types that the lexer will use to tokenize the input.
    
    """

    # Punctuation tokens
    # These tokens are used to represent the punctuation characters of the language
    LEFT_PAREN = 1
    RIGHT_PAREN = 2
    LEFT_BRACE = 3
    RIGHT_BRACE = 4
    LEFT_SQ_BRACK = 5
    RIGHT_SQ_BRACK = 6
    COMMA = 7
    SEMICOLON = 8

    # Special functions tokens
    # These tokens are used to represent the special functions of the language
    
    RANDOM_INT = 9
    PRINT = 10
    DELAY = 11
    WRITE_BOX = 12
    WRITE = 13

    # Keywords
    # These tokens are used to represent the keywords of the language
    AS = 14
    LET = 15
    RETURN = 16
    IF = 17
    ELSE = 18
    FOR = 19
    WHILE = 20
    FUN = 21

    # Identifiers
    # This token is used to represent the identifiers of the language
    IDENTIFIER = 22

    # Literals
    # These tokens are used to represent the literals of the language
    INT_LITERAL = 23
    FLOAT_LITERAL = 24
    BOOL_LITERAL = 25
    COLOR_LITERAL = 26
    WIDTH = 27
    HEIGHT = 28
    READ = 29


    # Skip tokens
    # This token is used to represent the tokens that the lexer will skip 
    SKIP = 30

    # Operators
    # These tokens are used to represent the operators of the language
    ADDITIVE_OP = 31
    MULTIPLICATIVE_OP = 32
    NOT_OP = 33
    ASSIGNMENT_OP = 34
    EQUAL_OP = 35
    NOT_EQUAL_OP = 36
    GREATER_OP = 37
    LESS_OP = 38
    GREATER_EQUAL_OP = 39
    LESS_EQUAL_OP = 40
    FUNC_ASSIGNMENT_OP = 41

    # End of file token
    # This token is used to represent the end of the file
    EOF = 42

    # Error token
    # This token is used to represent an error in the input
    ERROR = 43

# Token Class
class Token:

    """
    
    This class represents a token that will be generated by the lexer.
    
    """

    def __init__(self, TokenType, value):
        
        """
        
        This function initializes the token object.
        
        Parameters:
            TokenType (TokenType): The type of the token.
            value (str): The value of the token.
            
        """

        self.TokenType = TokenType
        self.value = value

def token_type_by_final_state(final_state, lexeme):

    """

    This function returns the token type that corresponds to the final state of the DFA.

    Parameters:
        final_state (int): The final state of the DFA.
        lexeme (str): The lexeme that the DFA has recognized.
    
    """

    match (final_state):
        case 1: 
            TokenType.ADDITIVE_OP
        case 2:
            TokenType.FUNC_ASSIGNMENT_OP
        case 3 if lexeme == "+":
            TokenType.ADDITIVE_OP
        case 3 if lexeme == "*":
            TokenType.MULTIPLICATIVE_OP
        case 4:
            TokenType.MULTIPLICATIVE_OP
        case 5:
            TokenType.SKIP
        case 6:
            TokenType.SKIP
        case 9:
            TokenType.ASSIGNMENT_OP 
        case 10 if lexeme == ">":
            TokenType.GREATER_OP
        case 10 if lexeme == "<":
            TokenType.LESS_OP
        case 12 if lexeme == "!=":
            TokenType.NOT_EQUAL_OP
        case 12 if lexeme == "==":
            TokenType.EQUAL_OP
        case 12 if lexeme == ">=":
            TokenType.GREATER_EQUAL_OP
        case 12 if lexeme == "<=":
            TokenType.LESS_EQUAL_OP
        case 13 if lexeme == "(":
            TokenType.LEFT_PAREN
        case 13 if lexeme == ")":
            TokenType.RIGHT_PAREN
        case 13 if lexeme == "{":
            TokenType.LEFT_BRACE
        case 13 if lexeme == "}":
            TokenType.RIGHT_BRACE
        case 13 if lexeme == "[":
            TokenType.LEFT_SQ_BRACK
        case 13 if lexeme == "]":
            TokenType.RIGHT_SQ_BRACK
        case 13 if lexeme == ",":
            TokenType.COMMA
        case 13 if lexeme == ";":
            TokenType.SEMICOLON
        case 16 if lexeme == "__width":
            TokenType.WIDTH
        case 16 if lexeme == "__height":
            TokenType.HEIGHT
        case 16 if lexeme == "__read":
            TokenType.READ
        case 16 if lexeme == "__random_int":
            TokenType.RANDOM_INT
        case 16 if lexeme == "__print":
            TokenType.PRINT
        case 16 if lexeme == "__delay":
            TokenType.DELAY
        case 16 if lexeme == "__write_box":
            TokenType.WRITE_BOX
        case 16 if lexeme == "__write":
            TokenType.WRITE
        case 16:
            TokenType.ERROR
        case 17 if lexeme == "as":
            TokenType.AS
        case 17 if lexeme == "let":
            TokenType.LET
        case 17 if lexeme == "return":
            TokenType.RETURN
        case 17 if lexeme == "if":
            TokenType.IF
        case 17 if lexeme == "else":
            TokenType.ELSE
        case 17 if lexeme == "for":
            TokenType.FOR
        case 17 if lexeme == "while":
            TokenType.WHILE
        case 17 if lexeme == "fun":
            TokenType.FUN
        case 17 if lexeme == "true" or lexeme == "false":
            TokenType.BOOL_LITERAL
        case 17 if lexeme == "and":
            TokenType.MULTIPLICATIVE_OP
        case 17 if lexeme == "or":
            TokenType.ADDITIVE_OP
        case 17 if lexeme == "not":
            TokenType.NOT_OP
        case 18: 
            TokenType.INT_LITERAL
        case 20:
            TokenType.FLOAT_LITERAL
        case 26:
            TokenType.COLOR_LITERAL