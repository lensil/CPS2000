"""

This file contains functionality related to tokens that the lexer will use to tokenize the input.

"""

from enum import Enum

class TokenType(Enum):

    """

    This class is an enumeration of all the token types that the lexer will use to tokenize the input.
    
    """

    # Punctuation tokens
    # These tokens are used to represent the punctuation characters of the language
    LEFT_PAREN = 1
    RIGHT_PAREN = 2
    LEFT_BRACE = 3
    RIGHT_BRACE = 4
    LEFT_SQ_BRACK = 5
    RIGHT_SQ_BRACK = 6
    COMMA = 7
    SEMICOLON = 8
    COLON = 9

    # Special functions tokens
    # These tokens are used to represent the special functions of the language
    RANDOM_INT = 10
    PRINT = 11
    DELAY = 12
    WRITE_BOX = 13
    WRITE = 14

    # Keywords
    # These tokens are used to represent the keywords of the language
    AS = 15
    LET = 16
    RETURN = 17
    IF = 18
    ELSE = 19
    FOR = 20
    WHILE = 21
    FUN = 22

    # Identifiers
    # This token is used to represent the identifiers of the language
    IDENTIFIER = 23

    # Literals
    # These tokens are used to represent the literals of the language
    INT_LITERAL = 24
    FLOAT_LITERAL = 25
    BOOL_LITERAL = 26
    COLOR_LITERAL = 27
    WIDTH = 28
    HEIGHT = 29
    READ = 30

    # These tokens are used to represent types of the language
    TYPE = 31

    # Skip tokens
    # This token is used to represent the tokens that the lexer will skip 
    SKIP = 32

    # Operators
    # These tokens are used to represent the operators of the language
    ADDITIVE_OP = 33
    MULTIPLICATIVE_OP = 34
    NOT_OP = 35
    ASSIGNMENT_OP = 36
    EQUAL_OP = 37
    RELATIONAL_OP = 38
    FUNC_ASSIGNMENT_OP = 39

    # End of file token
    # This token is used to represent the end of the file
    EOF = 40

    # Error token
    # This token is used to represent an error in the input
    ERROR = 41

# Token Class
class Token:

    """
    
    This class represents a token that will be generated by the lexer.
    
    """

    def __init__(self, TokenType, value, line):
        
        """
        
        This function initializes the token object.
        
        Parameters:
            TokenType (TokenType): The type of the token.
            value (str): The value of the token.
            line (int): The line number of the token.
            
        """

        self.TokenType = TokenType
        self.value = value
        self.line = line

def token_type_by_final_state(final_state, lexeme, line):

    """

    This function returns the token type that corresponds to the final state of the DFA.

    Parameters:
        final_state (int): The final state of the DFA.
        lexeme (str): The lexeme that the DFA has recognized.
        line (int): The line number of the lexeme.

    Returns:
        Token: The token that corresponds to the final state of the DFA.
    
    """

    # Match the final state of the DFA to return the corresponding token
    # The token type is determined by the final state of the DFA and in some cases by the lexeme
    
    match (final_state):
        case 1: 
            return Token(TokenType.ADDITIVE_OP, lexeme, line)
        case 2:
            return Token(TokenType.FUNC_ASSIGNMENT_OP, lexeme, line)
        case 3 if lexeme == "+":
            return Token(TokenType.ADDITIVE_OP, lexeme, line)
        case 3 if lexeme == "*":
            return Token(TokenType.MULTIPLICATIVE_OP, lexeme, line)
        case 4:
            return Token(TokenType.MULTIPLICATIVE_OP, lexeme, line)
        case 5:
            return Token(TokenType.SKIP, lexeme, line)
        case 6:
            return Token(TokenType.SKIP, lexeme, line)
        case 9:
            return Token(TokenType.ASSIGNMENT_OP, lexeme, line)
        case 10 if lexeme == ">":
            return Token(TokenType.RELATIONAL_OP, lexeme, line)
        case 10 if lexeme == "<":
            return Token(TokenType.RELATIONAL_OP, lexeme, line)
        case 12 if lexeme == "!=":
            return Token(TokenType.RELATIONAL_OP, lexeme, line)
        case 12 if lexeme == "==":
            return Token(TokenType.RELATIONAL_OP, lexeme, line)
        case 12 if lexeme == ">=":
            return Token(TokenType.RELATIONAL_OP, lexeme, line)
        case 12 if lexeme == "<=":
            return Token(TokenType.RELATIONAL_OP, lexeme, line)
        case 13 if lexeme == "(":
            return Token(TokenType.LEFT_PAREN, lexeme, line)
        case 13 if lexeme == ")":
            return Token(TokenType.RIGHT_PAREN, lexeme, line)
        case 13 if lexeme == "{":
            return Token(TokenType.LEFT_BRACE, lexeme, line)
        case 13 if lexeme == "}":
            return Token(TokenType.RIGHT_BRACE, lexeme, line)
        case 13 if lexeme == "[":
            return Token(TokenType.LEFT_SQ_BRACK, lexeme, line)
        case 13 if lexeme == "]":
            return Token(TokenType.RIGHT_SQ_BRACK, lexeme, line)
        case 13 if lexeme == ",":
            return Token(TokenType.COMMA, lexeme, line)
        case 13 if lexeme == ";":
            return Token(TokenType.SEMICOLON, lexeme, line)
        case 13 if lexeme == ":":
            return Token(TokenType.COLON, lexeme, line)
        case 16 if lexeme == "__width":
            return Token(TokenType.WIDTH, lexeme, line)
        case 16 if lexeme == "__height":
            return Token(TokenType.HEIGHT, lexeme, line)
        case 16 if lexeme == "__read":
            return Token(TokenType.READ, lexeme, line)
        case 16 if lexeme == "__random_int":
            return Token(TokenType.RANDOM_INT, lexeme, line)
        case 16 if lexeme == "__print":
            return Token(TokenType.PRINT, lexeme, line)
        case 16 if lexeme == "__delay":
            return Token(TokenType.DELAY, lexeme, line)
        case 16 if lexeme == "__write_box":
            return Token(TokenType.WRITE_BOX, lexeme, line)
        case 16 if lexeme == "__write":
            return Token(TokenType.WRITE, lexeme, line)
        case 17 if lexeme == "as":
            return Token(TokenType.AS, lexeme, line)
        # EXPAND ON THIS
        case 17 if lexeme == "float":
            return Token(TokenType.TYPE, lexeme, line)
        case 17 if lexeme == "int":
            return Token(TokenType.TYPE, lexeme, line)
        case 17 if lexeme == "bool":
            return Token(TokenType.TYPE, lexeme, line)
        case 17 if lexeme == "colour":
            return Token(TokenType.TYPE, lexeme, line)
        case 17 if lexeme == "let":
            return Token(TokenType.LET, lexeme, line)
        case 17 if lexeme == "return":
            return Token(TokenType.RETURN, lexeme, line)
        case 17 if lexeme == "if":
            return Token(TokenType.IF, lexeme, line)
        case 17 if lexeme == "else":
            return Token(TokenType.ELSE, lexeme, line)
        case 17 if lexeme == "for":
            return Token(TokenType.FOR, lexeme, line)
        case 17 if lexeme == "while":
            return Token(TokenType.WHILE, lexeme, line)
        case 17 if lexeme == "fun":
            return Token(TokenType.FUN, lexeme, line)
        case 17 if lexeme == "true" or lexeme == "false":
            return Token(TokenType.BOOL_LITERAL, lexeme, line)
        case 17 if lexeme == "and":
            return Token(TokenType.MULTIPLICATIVE_OP, lexeme, line)
        case 17 if lexeme == "or":
            return Token(TokenType.ADDITIVE_OP, lexeme, line)
        case 17 if lexeme == "not":
            return Token(TokenType.NOT_OP, lexeme, line)
        case 17:
            return Token(TokenType.IDENTIFIER, lexeme, line)
        case 18: 
            return Token(TokenType.INT_LITERAL, lexeme, line)
        case 20:
            return Token(TokenType.FLOAT_LITERAL, lexeme, line)
        case 27:
            return Token(TokenType.COLOR_LITERAL, lexeme, line)
        case _:
            raise Exception("Invalid token: " + lexeme)